---
layout: post
title: Building MiniTorch
description: My notes from completing Machine Learning Engineering at Cornell Tech
date: 2024-12-21
featured: true
toc:
  beginning: true
---

In Machine Learning Engineering (MLE), taught by Prof. Sasha Rush, I built [MiniTorch](https://minitorch.github.io/), a ground-up implementation of PyTorch. Starting from basis scalar operations, I implemented a fully operational Python library for training and using neural networks.

I think this was a super unique class; getting to focus on the _engineering_ side of Machine Learning Engineering was super cool. Now, when I call `torch.nn.Linear` or `loss.backward()`, I know what's happening under the hood. These are my key takeaways!

## Part 1: Fundamentals

<!-- First, we define some basic operators for scalar operations. As a simple example, we implement the ReLU function.

```python
def relu(x: float) -> float:
    """Compute the ReLU (Rectified Linear Unit) function."""
    return max(0.0, x)
```

Next, we consider how to implement a function that applies a scalar function to each element of an iterable. We do this by taking advantage of some functional programming: we define a `map` function that takes a function and returns a new function that applies the input function to each element of an iterable.

```python
def map(fn: Callable[[float], float]) -> Callable[[Iterable[float]], Iterable[float]]:
    """Apply a function to each element of an iterable.

    Args:
    ----
        fn: A function that takes a float and returns a float.

    Returns:
    -------
        A function that takes an iterable of floats and returns an iterable of floats
        with the input function applied to each element.

    """

    def inner(ls: Iterable[float]) -> Iterable[float]:
        return [fn(x) for x in ls]

    return inner
``` -->

Our goal is to implement code that allows us to train large neural networks. This requires keeping track of many parameters, as well as tracing operations to compute gradients.

We do this by using a `Module` building block that is maintained in a tree structure. Each module stores its own parameters, as well as its submodules.

Why a tree structure?

- Trees naturally represent hierarchical structures, which are common neural networks (layers building on top of each other)
- Trees allow us to easily represent parallel operations
- We can easily isolate and debug specific parts of the network by tracing through the tree

For our base `Module` class, we define attributes

- `_modules`: Storage of the child modules
- `_parameters`: Storage of the module's parameters
- `training`: Whether the module is in training mode or evaluation mode

This class will also have functions to collect named parameters, submodules, etc. Another note:
we use magic methods to override the default behavior of Python, such as custom `getattr` and `setattr` methods. We also have a `__call__` method that invokes any forward method we define.

With this Module class, we can extend it to create our own custom modules. For example, we can create a `OtherModule` class that inherits from `Module`.

```python
class OtherModule(Module):
    def __init__(self):
        # Must initialize the super class!
        super().__init__()
        self.uncool_parameter = Parameter(60)


class MyModule(Module):
    def __init__(self):
        # Must initialize the super class!
        super().__init__()

        # Type 1, a parameter.
        self.parameter1 = Parameter(15)
        self.cool_parameter = Parameter(50)

        # Type 2, user data
        self.data = 25

        # Type 3. another Module
        self.sub_module_a = OtherModule()
        self.sub_module_b = OtherModule()
```

This is a simple toy example, but it illustrates how we might define useful modules like `Linear`, `Conv2d`, etc.

## Part 2: Autodiff

The next part of the class is all about autodifferentiation: how do we actually compute and store gradients for training?

## Part 3: Tensors

## Part 4: Efficiency
