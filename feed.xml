<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://evan-wang-13.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://evan-wang-13.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-12-31T19:43:31+00:00</updated><id>https://evan-wang-13.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Building MiniTorch</title><link href="https://evan-wang-13.github.io/blog/2024/MLE/" rel="alternate" type="text/html" title="Building MiniTorch"/><published>2024-12-21T00:00:00+00:00</published><updated>2024-12-21T00:00:00+00:00</updated><id>https://evan-wang-13.github.io/blog/2024/MLE</id><content type="html" xml:base="https://evan-wang-13.github.io/blog/2024/MLE/"><![CDATA[<p>In Machine Learning Engineering (MLE), taught by Prof. Sasha Rush, I built <a href="https://minitorch.github.io/">MiniTorch</a>, a ground-up implementation of PyTorch. Starting from basis scalar operations, I implemented a fully operational Python library for training and using neural networks.</p> <p>Getting to focus on the <em>engineering</em> side of Machine Learning Engineering was super cool. Now, when I call <code class="language-plaintext highlighter-rouge">torch.nn.Linear</code> or <code class="language-plaintext highlighter-rouge">loss.backward()</code>, I know what’s happening under the hood. These are my key takeaways! Note that I won’t be discussing the fundamental theory behind ML (e.g. why we want to find the gradient of the loss function), as this blog focuses on the <em>why</em> and <em>how</em> of engineering ML code.</p> <h2 id="part-1-fundamentals">Part 1: Fundamentals</h2> <p>Our goal is to implement code that allows us to train large neural networks. This requires keeping track of many parameters, as well as tracing operations to compute gradients.</p> <p>We do this by using a <code class="language-plaintext highlighter-rouge">Module</code> building block that is maintained in a tree structure. Each module stores its own parameters, as well as its submodules.</p> <p>Why a tree structure?</p> <ul> <li>Trees naturally represent hierarchical structures, which are common neural networks (layers building on top of each other)</li> <li>Trees allow us to easily represent parallel operations</li> <li>Components and their owned parameters are clearly defined, which is useful for debugging, memory management, gradient tracking etc.</li> </ul> <p>For our base <code class="language-plaintext highlighter-rouge">Module</code> class, we define attributes</p> <ul> <li><code class="language-plaintext highlighter-rouge">_modules</code>: Storage of the child modules</li> <li><code class="language-plaintext highlighter-rouge">_parameters</code>: Storage of the module’s parameters</li> <li><code class="language-plaintext highlighter-rouge">training</code>: Whether the module is in training mode or evaluation mode</li> </ul> <p>This class will also have functions to collect named parameters, submodules, etc. Another note: we use magic methods to override the default behavior of Python, such as custom <code class="language-plaintext highlighter-rouge">getattr</code> and <code class="language-plaintext highlighter-rouge">setattr</code> methods. We also have a <code class="language-plaintext highlighter-rouge">__call__</code> method that invokes any forward method we define.</p> <p>With this Module class, we can extend it to create our own custom modules. For example, we can create a <code class="language-plaintext highlighter-rouge">OtherModule</code> class that inherits from <code class="language-plaintext highlighter-rouge">Module</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">OtherModule</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="c1"># Must initialize the super class!
</span>        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">uncool_parameter</span> <span class="o">=</span> <span class="nc">Parameter</span><span class="p">(</span><span class="mi">60</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">MyModule</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="c1"># Must initialize the super class!
</span>        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="c1"># Parameters.
</span>        <span class="n">self</span><span class="p">.</span><span class="n">parameter1</span> <span class="o">=</span> <span class="nc">Parameter</span><span class="p">(</span><span class="mi">15</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">cool_parameter</span> <span class="o">=</span> <span class="nc">Parameter</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>

        <span class="c1"># User data
</span>        <span class="n">self</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="mi">25</span>

        <span class="c1"># Submodules
</span>        <span class="n">self</span><span class="p">.</span><span class="n">sub_module_a</span> <span class="o">=</span> <span class="nc">OtherModule</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">sub_module_b</span> <span class="o">=</span> <span class="nc">OtherModule</span><span class="p">()</span>
</code></pre></div></div> <p>This is a simple toy example, but it illustrates how we might define useful modules like <code class="language-plaintext highlighter-rouge">Linear</code>, <code class="language-plaintext highlighter-rouge">Conv2d</code>, etc. In practice, this could look like a a ResNet module that contains convolutional and lienar submodules, with these submodules containing their own parameters (model weights).</p> <h2 id="part-2-autodiff">Part 2: Autodiff</h2> <p>The next part of the class is all about differentiation: how do we actually compute and store gradients for training?</p> <p>One approach we can take is symbolic differentiation, where we explicitly find the mathematical expression for the gradient of the function (as we would do in calculus class). This makes sense for simple functions like \(f(x) = x^2\), but becomes impractical for the more complex functions that would represent a neural network. Solving and storing such an expression explodes with model size.</p> <p>Another approach is numerical differentiation, where we approximate the gradient by altering inputs slightly and observing the change in output. The issue here is a) numerical instability due to approximation errors and b) we would need to compute the gradient for each parameter. We can imagine that for contemporary models with billions of parameters, this is impractical.</p> <p>To address these issues, automatic differentiation (autodiff) is the approach used in practice, which essentially collects information about every operation we perform in the forward pass, and uses this information to compute gradients in the backward pass. We can represent this collection and flow of information as a graph. I’ve seen nodes as functions and edges as values as well as the opposite; both perspectives are valid and the graph is simply a representation of how we organize the information.</p> <p>With autodiff, we compute exact gradients like with symbolic differentiation, but we only need to evaluate each operation once (unlike numerical differentiation), since we store intermediate gradient accumulation values rather than entire expressions. Thus, autodiff is precise and scalable.</p> <p>To do so in Python, we must override the default behavior of numbers and operators. To this end, we define proxy <code class="language-plaintext highlighter-rouge">Scalar</code> and <code class="language-plaintext highlighter-rouge">ScalarFunction</code> classes that allow us to remember what operators were used on what numbers.</p> <p>As an example, see how we define the multiplication operation. Note the usage of the context (ctx) variable to store values necessary for backpropagation:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Mul</span><span class="p">(</span><span class="n">ScalarFunction</span><span class="p">):</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">ctx</span><span class="p">.</span><span class="nf">save_for_backward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
        <span class="c1"># Compute f'_x(x, y) * d, f'_y(x, y) * d
</span>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">ctx</span><span class="p">.</span><span class="n">saved_values</span>
        <span class="n">f_x_prime</span> <span class="o">=</span> <span class="n">y</span>
        <span class="n">f_y_prime</span> <span class="o">=</span> <span class="n">x</span>
        <span class="k">return</span> <span class="n">f_x_prime</span> <span class="o">*</span> <span class="n">d</span><span class="p">,</span> <span class="n">f_y_prime</span> <span class="o">*</span> <span class="n">d</span>
</code></pre></div></div> <p>With this data saved, we can define a backpropagate function that actually computes the gradients. Note the usage of topological sort to obtain the order of operations. We also use the chain rule to calculate the gradients as we traverse the graph.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">backpropagate</span><span class="p">(</span><span class="n">variable</span><span class="p">:</span> <span class="n">Variable</span><span class="p">,</span> <span class="n">deriv</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Runs backpropagation on the computation graph in order to
    compute derivatives for the leave nodes.

    Args:
    ----
        variable: The right-most variable
        deriv  : Its derivative that we want to propagate backward to the leaves.

    Returns:
    -------
    No return. Should write to its results to the derivative values of each leaf through `accumulate_derivative`.

    </span><span class="sh">"""</span>
    <span class="c1"># Initialize a dictionary to store derivatives for each variable
</span>    <span class="n">derivatives</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="c1"># Perform topological sort
</span>    <span class="n">sorted_variables</span> <span class="o">=</span> <span class="nf">topological_sort</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span>
    <span class="n">derivatives</span><span class="p">[</span><span class="n">variable</span><span class="p">.</span><span class="n">unique_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">deriv</span>
    <span class="c1"># Iterate through the sorted variables
</span>    <span class="k">for</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">sorted_variables</span><span class="p">:</span>
        <span class="c1"># Get the current derivative for this variable
</span>        <span class="n">d_output</span> <span class="o">=</span> <span class="n">derivatives</span><span class="p">[</span><span class="n">var</span><span class="p">.</span><span class="n">unique_id</span><span class="p">]</span>

        <span class="c1"># If it's a leaf node, accumulate the derivative
</span>        <span class="k">if</span> <span class="n">var</span><span class="p">.</span><span class="nf">is_leaf</span><span class="p">():</span>
            <span class="n">var</span><span class="p">.</span><span class="nf">accumulate_derivative</span><span class="p">(</span><span class="n">d_output</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Apply the chain rule to get derivatives for parent variables
</span>            <span class="k">for</span> <span class="n">parent</span><span class="p">,</span> <span class="n">grad</span> <span class="ow">in</span> <span class="n">var</span><span class="p">.</span><span class="nf">chain_rule</span><span class="p">(</span><span class="n">d_output</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">parent</span><span class="p">.</span><span class="n">unique_id</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">derivatives</span><span class="p">:</span>
                    <span class="n">derivatives</span><span class="p">[</span><span class="n">parent</span><span class="p">.</span><span class="n">unique_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">grad</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">derivatives</span><span class="p">[</span><span class="n">parent</span><span class="p">.</span><span class="n">unique_id</span><span class="p">]</span> <span class="o">+=</span> <span class="n">grad</span>
</code></pre></div></div> <h2 id="part-3-tensors">Part 3: Tensors</h2> <p>So far, we’ve been working with scalars. But for neural networks with many parameters and weights, reasoning about each individual parameter is impractical. Instead, we define a new data structure: <code class="language-plaintext highlighter-rouge">tensors</code>, which are multi-dimensional arrays that can serve as a basis for our mathematical programming.</p> <p>Why don’t we just use lists?</p> <ul> <li>We can define functions to manipulate shape</li> <li>Enables autodiff</li> <li>Enables control of memory</li> </ul> <p>Tensors have some properties: <code class="language-plaintext highlighter-rouge">dims</code>, <code class="language-plaintext highlighter-rouge">size</code>, <code class="language-plaintext highlighter-rouge">shape</code>. <code class="language-plaintext highlighter-rouge">dims</code> is the number of dimensions, <code class="language-plaintext highlighter-rouge">size</code> is the total number of elements, and <code class="language-plaintext highlighter-rouge">shape</code> is the size of each dimension.</p> <p>What are the cons of working with tensors?</p> <ul> <li>Hard to grow and shrink</li> <li>Lose python built-ins</li> <li>Tricky to reason about shapes</li> </ul> <p>Important note on the internals of tensors: we have a <code class="language-plaintext highlighter-rouge">Storage</code> that stores the actual data, and a <code class="language-plaintext highlighter-rouge">Tensor</code> class that wraps around the storage and provides a Python interface for interacting with the data.</p> <p>The <code class="language-plaintext highlighter-rouge">Storage</code> is a 1-D array of numbers of length <code class="language-plaintext highlighter-rouge">size</code>, and we have a <code class="language-plaintext highlighter-rouge">Strides</code> tuple that provides the mapping from user indices to indices in this 1-D storage. So, when we reshape or take transposes of tensors, the underlying data is not actually changed. Instead, our strides change.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/3dtensor-480.webp 480w,/assets/img/3dtensor-800.webp 800w,/assets/img/3dtensor-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/3dtensor.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Take this example of a 3D tensor (<code class="language-plaintext highlighter-rouge">dims</code> is 3, <code class="language-plaintext highlighter-rouge">size</code> is 12). We have shape <code class="language-plaintext highlighter-rouge">(2, 2, 3)</code>, and we have a <code class="language-plaintext highlighter-rouge">Strides</code> tuple <code class="language-plaintext highlighter-rouge">(6, 3, 1)</code>.</p> <p>We follow a convention where tensor shapes are specified as (depth, rows, columns). The strides array tells us how to map these dimensions to memory: a stride of 6 means moving one position in depth skips 6 storage positions, a stride of 3 means moving one row skips 3 storage positions, and a stride of 1 means moving one column skips a single storage position.</p> <h2 id="part-4-efficiency">Part 4: Efficiency</h2> <p>Modern deep learning calls for large-scale training and inference, on the order of billions of parameters. To this end, we must make our code for each operation as efficient as possible.How do we increase efficiency? To tackle this problem, let’s first consider why our Python code may be slow in the first place!</p> <ul> <li>Function calls <ul> <li>Function calls are not free: must check for args, special keywords, overrides, class inheritance, etc.</li> </ul> </li> <li>Type checking <ul> <li>Python is dynamically typed, so we must check types at runtime. May need to cast to correct type, or raise error, or call different functions depending on type</li> </ul> </li> <li>Loops <ul> <li>Loops are run as is, meaning it’s hard to parallelize or pull out constant computations</li> </ul> </li> </ul> <p>The first approach we take to tackling these bottlenecks is by using Numba, a just-in-time (JIT) compiler for Python that compiles Python bytecode to machine code at runtime.</p> <p>JIT means that the compiler waits until the function is called to compile the code. This means that the compiler has access to information that wasn’t avilable during ahead-of-time compilation, such as data types. Thus, Numba can generate optimzed machine code, eliminating the need for Python’s dynamic type checking and handling. This allows us to use Python syntax and still get performance close to C++.</p> <p>What do we lose with Numba? - Not all Python features are supported</p> <ul> <li>The first run of code will be slower due to compilation</li> <li>Only works with a subset of NumPy functions</li> <li>Complex data structures like dicts or custom classes may not be supported</li> </ul> <p>As an example of how we can use Numba, let’s consider summing the elements of an array.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Traditional Python
</span><span class="k">def</span> <span class="nf">python_sum_array</span><span class="p">(</span><span class="n">arr</span><span class="p">):</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">arr</span><span class="p">)):</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">arr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">total</span>

<span class="c1"># Numba-optimized
</span><span class="nd">@njit</span>
<span class="k">def</span> <span class="nf">numba_sum_array</span><span class="p">(</span><span class="n">arr</span><span class="p">):</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">prange</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">arr</span><span class="p">)):</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">arr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">total</span>

<span class="n">fast_my_code</span> <span class="o">=</span> <span class="n">numba</span><span class="p">.</span><span class="nf">njit</span><span class="p">(</span><span class="n">parallel</span><span class="o">=</span><span class="bp">True</span><span class="p">)(</span><span class="n">my_code</span><span class="p">)</span>
</code></pre></div></div> <p>the prange in the numba code tells the compiler to run the code on multiple threads, parallelizing the loop. The compiler now knows it can run the loops in any order, which is more efficient but also means we must be careful for operations that require ordering</p> <p>Now, let’s move onto NVIDIA’s powerful programming language for improving efficiency: CUDA.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[My notes from completing Machine Learning Engineering at Cornell Tech]]></summary></entry><entry><title type="html">Limits to Prediction</title><link href="https://evan-wang-13.github.io/blog/2024/LimitsToPrediction/" rel="alternate" type="text/html" title="Limits to Prediction"/><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://evan-wang-13.github.io/blog/2024/LimitsToPrediction</id><content type="html" xml:base="https://evan-wang-13.github.io/blog/2024/LimitsToPrediction/"><![CDATA[<p>This spring, I took my favorite class at Princeton: the seminar <a href="https://msalganik.github.io/soc555-cos598J_s2024/">Limits to Prediction</a>, taught by Arvind Narayanan and Matt Salganik. Limits to Prediction featured a lecture-discussion format where both professors, along with students, discussed and responded to presented material. The widely applicable takeaways–ranging from why we make predictions to the pitfalls of AI benchmarking–were extremely impactful for me. Here are my notes that I want to keep in mind throughout my academic career and beyond.</p> <h2 id="meta-level-choosing-what-to-learnresearch">Meta-Level: Choosing What to Learn/Research</h2> <ul> <li>Choosing what to focus on in learning and research is a prediction task in and of itself. What will be super important in the next few years (or decades) that is not receiving enough attention already?</li> </ul> <p>A tangential discussion is the concept of strongly-held weak beliefs vs. weakly-held strong beliefs. Strongly-held weak beliefs are hypotheses that may not push the envelope super far from established knowledge, but we can be relatively confident in that belief. These beliefs are more foundational than ground-breaking. Weakly-held strong beliefs are beliefs that are “hot takes” but have low confidence. Some examples: Matt has a strongly-held weak belief that for a domain like music, randomness and luck can significantly determine how popular a song is-merit and quality is not a fool-proof determiner of success. Arvind has a weakly-held strong belief that the issues with LLM evaluation (contamination, prompt sensitivity, robustness, and more that we will get into in this post) are not fixable, and thus future work should be oriented towards replacing benchmarking rather than fixing it.</p> <p>Usual discourse pits these against each other-which should an aspiring researcher follow? This may be true at an individual level, but they certainly are not mutually exclusive at a larger scale. Both types of beliefs are important to have in the marketplace of ideas: diversity here is healthy.</p> <p>Matt describes himself as someone having strongly-held weak beliefs, while Arvind considers himself as having weakly-held strong beliefs.</p> <h2 id="evaluations-are-everything">Evaluations are Everything</h2> <ul> <li>Data Leakage and the <a href="https://arxiv.org/pdf/math.ST/0606441">ping-pong theorem</a>. You can read more about data leakage <a href="https://reproducible.cs.princeton.edu/">here</a>.</li> <li>Crucially, there is often a mismatch between training loss, performance evaluation, and what actually matters post-deployment. For example, say we are using some classifcation model. We might use maximum likelihood estimation to find the models’ parameters, some misclassification rate to evaluate the model, but in practice, what truly matters is a particular cost-weighted misclassification rate. It should be imperative to take time and ensure coherency here.</li> </ul> <p>For a particularly illuminating example, let’s look at the discussion over emergent abilities of LLMS.</p> <p>2022 paper evaluated LLM performance on some popular benchmarks as a function of parameter size. Contrary to traditional scaling laws, models achieve random performance up to a certain scale, then improves-sharply and unpredictably. This is a pretty crucial finding. If models truly exhibit emergent abilities, that makes predicting their predictive power, abilities, and risks much harder. A bunch of steps and research would probably need to follow.</p> <p>But! A 2023 <a href="https://arxiv.org/abs/2304.15004">paper</a> suggests this is not the full picture. “emergent abilities seem to appear only under metrics that nonlinearly or discontinuously scale any model’s per-token error rate.”</p> <p>Results of testing this hypothesis are shown below, taken from their paper. For completeness, the first column follows a simple mathematical model, where they assume cross entropy loss decreases monotonically with scale, and thus per-token probability of selecting the right token asymptotes towards 1.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/emergentMirage-480.webp 480w,/assets/img/emergentMirage-800.webp 800w,/assets/img/emergentMirage-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/emergentMirage.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>When we go from a non-linear metric, where the scoring tends to be more binary with a strict threshold, to a non-linear metric, the emergent pattern seems to disappear. So what is the “correct” metric to use? It probably doesn’t matter that much-ultimately whatever allows us to make the best predictions for the desired task. But, overall, this recasts emergence from a model property to something about how we evaluate/consider knowledge.</p> ]]></content><author><name></name></author><summary type="html"><![CDATA[What I learned from my favorite class at Princeton. Will continue to update as I parse through my notes.]]></summary></entry></feed>