<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Building MiniTorch | Evan Wang </title> <meta name="author" content="Evan Wang"> <meta name="description" content="My notes from completing Machine Learning Engineering at Cornell Tech"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="/assets/img/brain.png?1dc272eec2f759dc010e77d5a4e69b53"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://evan-wang-13.github.io/blog/2024/MLE/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Evan</span> Wang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Building MiniTorch</h1> <p class="post-meta"> December 21, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a> </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h2"><a href="#part-1-fundamentals">Part 1: Fundamentals</a></li> <li class="toc-entry toc-h2"><a href="#part-2-autodiff">Part 2: Autodiff</a></li> <li class="toc-entry toc-h2"><a href="#part-3-tensors">Part 3: Tensors</a></li> <li class="toc-entry toc-h2"><a href="#part-4-efficiency">Part 4: Efficiency</a></li> </ul> </div> <hr> <div id="markdown-content"> <p>In Machine Learning Engineering (MLE), taught by Prof. Sasha Rush, I built <a href="https://minitorch.github.io/" rel="external nofollow noopener" target="_blank">MiniTorch</a>, a ground-up implementation of PyTorch. Starting from basis scalar operations, I implemented a fully operational Python library for training and using neural networks.</p> <p>Getting to focus on the <em>engineering</em> side of Machine Learning Engineering was super cool. Now, when I call <code class="language-plaintext highlighter-rouge">torch.nn.Linear</code> or <code class="language-plaintext highlighter-rouge">loss.backward()</code>, I know what’s happening under the hood. These are my key takeaways! Note that I won’t be discussing the fundamental theory behind ML (e.g. why we want to find the gradient of the loss function), as this blog focuses on the <em>why</em> and <em>how</em> of engineering ML code.</p> <h2 id="part-1-fundamentals">Part 1: Fundamentals</h2> <p>Our goal is to implement code that allows us to train large neural networks. This requires keeping track of many parameters, as well as tracing operations to compute gradients.</p> <p>We do this by using a <code class="language-plaintext highlighter-rouge">Module</code> building block that is maintained in a tree structure. Each module stores its own parameters, as well as its submodules.</p> <p>Why a tree structure?</p> <ul> <li>Trees naturally represent hierarchical structures, which are common neural networks (layers building on top of each other)</li> <li>Trees allow us to easily represent parallel operations</li> <li>Components and their owned parameters are clearly defined, which is useful for debugging, memory management, gradient tracking etc.</li> </ul> <p>For our base <code class="language-plaintext highlighter-rouge">Module</code> class, we define attributes</p> <ul> <li> <code class="language-plaintext highlighter-rouge">_modules</code>: Storage of the child modules</li> <li> <code class="language-plaintext highlighter-rouge">_parameters</code>: Storage of the module’s parameters</li> <li> <code class="language-plaintext highlighter-rouge">training</code>: Whether the module is in training mode or evaluation mode</li> </ul> <p>This class will also have functions to collect named parameters, submodules, etc. Another note: we use magic methods to override the default behavior of Python, such as custom <code class="language-plaintext highlighter-rouge">getattr</code> and <code class="language-plaintext highlighter-rouge">setattr</code> methods. We also have a <code class="language-plaintext highlighter-rouge">__call__</code> method that invokes any forward method we define.</p> <p>With this Module class, we can extend it to create our own custom modules. For example, we can create a <code class="language-plaintext highlighter-rouge">OtherModule</code> class that inherits from <code class="language-plaintext highlighter-rouge">Module</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">OtherModule</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="c1"># Must initialize the super class!
</span>        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">uncool_parameter</span> <span class="o">=</span> <span class="nc">Parameter</span><span class="p">(</span><span class="mi">60</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">MyModule</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="c1"># Must initialize the super class!
</span>        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="c1"># Parameters.
</span>        <span class="n">self</span><span class="p">.</span><span class="n">parameter1</span> <span class="o">=</span> <span class="nc">Parameter</span><span class="p">(</span><span class="mi">15</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">cool_parameter</span> <span class="o">=</span> <span class="nc">Parameter</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>

        <span class="c1"># User data
</span>        <span class="n">self</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="mi">25</span>

        <span class="c1"># Submodules
</span>        <span class="n">self</span><span class="p">.</span><span class="n">sub_module_a</span> <span class="o">=</span> <span class="nc">OtherModule</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">sub_module_b</span> <span class="o">=</span> <span class="nc">OtherModule</span><span class="p">()</span>
</code></pre></div></div> <p>This is a simple toy example, but it illustrates how we might define useful modules like <code class="language-plaintext highlighter-rouge">Linear</code>, <code class="language-plaintext highlighter-rouge">Conv2d</code>, etc. In practice, this could look like a a ResNet module that contains convolutional and lienar submodules, with these submodules containing their own parameters (model weights).</p> <h2 id="part-2-autodiff">Part 2: Autodiff</h2> <p>The next part of the class is all about differentiation: how do we actually compute and store gradients for training?</p> <p>One approach we can take is symbolic differentiation, where we explicitly find the mathematical expression for the gradient of the function (as we would do in calculus class). This makes sense for simple functions like \(f(x) = x^2\), but becomes impractical for the more complex functions that would represent a neural network. Solving and storing such an expression explodes with model size.</p> <p>Another approach is numerical differentiation, where we approximate the gradient by altering inputs slightly and observing the change in output. The issue here is a) numerical instability due to approximation errors and b) we would need to compute the gradient for each parameter. We can imagine that for contemporary models with billions of parameters, this is impractical.</p> <p>To address these issues, automatic differentiation (autodiff) is the approach used in practice, which essentially collects information about every operation we perform in the forward pass, and uses this information to compute gradients in the backward pass. We can represent this collection and flow of information as a graph. With autodiff, we compute exact gradients like with symbolic differentiation, but we only need to evaluate each operation once (unlike numerical differentiation), since we store intermediate gradient accumulation values rather than entire expressions. Thus, autodiff is precise and scalable.</p> <pre><code class="language-mermaid">graph LR
    x1[x = 2.0] --&gt; mul1[×]
    x2[x = 2.0] --&gt; mul1
    mul1 --&gt; add1[+]
    x3[x = 2.0] --&gt; mul2[×]
    x4[x = 2.0] --&gt; mul2
    mul2 --&gt; add1
    add1 --&gt; result[y = 8.0]

    %% Add gradient flow
    result -- ∂y/∂+ = 1.0 --&gt; add1
    add1 -- ∂y/∂× = 1.0 --&gt; mul1
    add1 -- ∂y/∂× = 1.0 --&gt; mul2
    mul1 -- ∂y/∂x = 2.0 --&gt; x1
    mul1 -- ∂y/∂x = 2.0 --&gt; x2
    mul2 -- ∂y/∂x = 2.0 --&gt; x3
    mul2 -- ∂y/∂x = 2.0 --&gt; x4
</code></pre> <p>To do so in Python, we must override the default behavior of numbers and operators. To this end, we define proxy <code class="language-plaintext highlighter-rouge">Scalar</code> and <code class="language-plaintext highlighter-rouge">ScalarFunction</code> classes that allow us to remember what operators were used on what numbers.</p> <p>As an example, see how we define the multiplication operation:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Mul</span><span class="p">(</span><span class="n">ScalarFunction</span><span class="p">):</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">ctx</span><span class="p">.</span><span class="nf">save_for_backward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
        <span class="c1"># Compute f'_x(x, y) * d, f'_y(x, y) * d
</span>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">ctx</span><span class="p">.</span><span class="n">saved_values</span>
        <span class="n">f_x_prime</span> <span class="o">=</span> <span class="n">y</span>
        <span class="n">f_y_prime</span> <span class="o">=</span> <span class="n">x</span>
        <span class="k">return</span> <span class="n">f_x_prime</span> <span class="o">*</span> <span class="n">d</span><span class="p">,</span> <span class="n">f_y_prime</span> <span class="o">*</span> <span class="n">d</span>
</code></pre></div></div> <h2 id="part-3-tensors">Part 3: Tensors</h2> <p>Data structures for storing and manipulating data.</p> <h2 id="part-4-efficiency">Part 4: Efficiency</h2> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/LimitsToPrediction/">Limits to Prediction</a> </li> </div> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2024 Evan Wang. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script>let theme=determineComputedTheme();document.onreadystatechange=(()=>{"complete"===document.readyState&&(document.querySelectorAll("pre>code.language-mermaid").forEach(e=>{const t=e.textContent,d=e.parentElement;d.classList.add("unloaded");let n=document.createElement("pre");n.classList.add("mermaid");const a=document.createTextNode(t);n.appendChild(a),d.after(n)}),mermaid.initialize({theme:theme}),"undefined"!=typeof d3&&window.addEventListener("load",function(){d3.selectAll(".mermaid svg").each(function(){var e=d3.select(this);e.html("<g>"+e.html()+"</g>");var t=e.select("g"),d=d3.zoom().on("zoom",function(e){t.attr("transform",e.transform)});e.call(d)})}))});</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>