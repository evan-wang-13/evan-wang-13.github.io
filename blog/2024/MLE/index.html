<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Building MiniTorch | Evan Wang </title> <meta name="author" content="Evan Wang"> <meta name="description" content="My notes from completing Machine Learning Engineering at Cornell Tech"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="/assets/img/brain.png?1dc272eec2f759dc010e77d5a4e69b53"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://evan-wang-13.github.io/blog/2024/MLE/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Evan</span> Wang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Building MiniTorch</h1> <p class="post-meta"> December 21, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a> </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h2"><a href="#part-1-fundamentals">Part 1: Fundamentals</a></li> <li class="toc-entry toc-h2"><a href="#part-2-autodiff">Part 2: Autodiff</a></li> <li class="toc-entry toc-h2"><a href="#part-3-tensors">Part 3: Tensors</a></li> <li class="toc-entry toc-h2"><a href="#part-4-efficiency">Part 4: Efficiency</a></li> </ul> </div> <hr> <div id="markdown-content"> <p>In Machine Learning Engineering (MLE), taught by Prof. Sasha Rush, I built <a href="https://minitorch.github.io/" rel="external nofollow noopener" target="_blank">MiniTorch</a>, a ground-up implementation of PyTorch. Starting from basis scalar operations, I implemented a fully operational Python library for training and using neural networks.</p> <p>Getting to focus on the <em>engineering</em> side of Machine Learning Engineering was super cool. Now, when I call <code class="language-plaintext highlighter-rouge">torch.nn.Linear</code> or <code class="language-plaintext highlighter-rouge">loss.backward()</code>, I know what’s happening under the hood. These are my key takeaways! Note that I won’t be discussing the fundamental theory behind ML (e.g. why we want to find the gradient of the loss function), as this blog focuses on the <em>why</em> and <em>how</em> of engineering ML code.</p> <h2 id="part-1-fundamentals">Part 1: Fundamentals</h2> <p>Our goal is to implement code that allows us to train large neural networks. This requires keeping track of many parameters, as well as tracing operations to compute gradients.</p> <p>We do this by using a <code class="language-plaintext highlighter-rouge">Module</code> building block that is maintained in a tree structure. Each module stores its own parameters, as well as its submodules.</p> <p>Why a tree structure?</p> <ul> <li>Trees naturally represent hierarchical structures, which are common neural networks (layers building on top of each other)</li> <li>Trees allow us to easily represent parallel operations</li> <li>Components and their owned parameters are clearly defined, which is useful for debugging, memory management, gradient tracking etc.</li> </ul> <p>For our base <code class="language-plaintext highlighter-rouge">Module</code> class, we define attributes</p> <ul> <li> <code class="language-plaintext highlighter-rouge">_modules</code>: Storage of the child modules</li> <li> <code class="language-plaintext highlighter-rouge">_parameters</code>: Storage of the module’s parameters</li> <li> <code class="language-plaintext highlighter-rouge">training</code>: Whether the module is in training mode or evaluation mode</li> </ul> <p>This class will also have functions to collect named parameters, submodules, etc. Another note: we use magic methods to override the default behavior of Python, such as custom <code class="language-plaintext highlighter-rouge">getattr</code> and <code class="language-plaintext highlighter-rouge">setattr</code> methods. We also have a <code class="language-plaintext highlighter-rouge">__call__</code> method that invokes any forward method we define.</p> <p>With this Module class, we can extend it to create our own custom modules. For example, we can create a <code class="language-plaintext highlighter-rouge">OtherModule</code> class that inherits from <code class="language-plaintext highlighter-rouge">Module</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">OtherModule</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="c1"># Must initialize the super class!
</span>        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">uncool_parameter</span> <span class="o">=</span> <span class="nc">Parameter</span><span class="p">(</span><span class="mi">60</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">MyModule</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="c1"># Must initialize the super class!
</span>        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="c1"># Parameters.
</span>        <span class="n">self</span><span class="p">.</span><span class="n">parameter1</span> <span class="o">=</span> <span class="nc">Parameter</span><span class="p">(</span><span class="mi">15</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">cool_parameter</span> <span class="o">=</span> <span class="nc">Parameter</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>

        <span class="c1"># User data
</span>        <span class="n">self</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="mi">25</span>

        <span class="c1"># Submodules
</span>        <span class="n">self</span><span class="p">.</span><span class="n">sub_module_a</span> <span class="o">=</span> <span class="nc">OtherModule</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">sub_module_b</span> <span class="o">=</span> <span class="nc">OtherModule</span><span class="p">()</span>
</code></pre></div></div> <p>This is a simple toy example, but it illustrates how we might define useful modules like <code class="language-plaintext highlighter-rouge">Linear</code>, <code class="language-plaintext highlighter-rouge">Conv2d</code>, etc. In practice, this could look like a a ResNet module that contains convolutional and lienar submodules, with these submodules containing their own parameters (model weights).</p> <h2 id="part-2-autodiff">Part 2: Autodiff</h2> <p>The next part of the class is all about differentiation: how do we actually compute and store gradients for training?</p> <p>One approach we can take is symbolic differentiation, where we explicitly find the mathematical expression for the gradient of the function (as we would do in calculus class). This makes sense for simple functions like \(f(x) = x^2\), but becomes impractical for the more complex functions that would represent a neural network. Solving and storing such an expression explodes with model size.</p> <p>Another approach is numerical differentiation, where we approximate the gradient by altering inputs slightly and observing the change in output. The issue here is a) numerical instability due to approximation errors and b) we would need to compute the gradient for each parameter. We can imagine that for contemporary models with billions of parameters, this is impractical.</p> <p>To address these issues, automatic differentiation (autodiff) is the approach used in practice, which essentially collects information about every operation we perform in the forward pass, and uses this information to compute gradients in the backward pass. We can represent this collection and flow of information as a graph. I’ve seen nodes as functions and edges as values as well as the opposite; both perspectives are valid and the graph is simply a representation of how we organize the information.</p> <p>With autodiff, we compute exact gradients like with symbolic differentiation, but we only need to evaluate each operation once (unlike numerical differentiation), since we store intermediate gradient accumulation values rather than entire expressions. Thus, autodiff is precise and scalable.</p> <p>To do so in Python, we must override the default behavior of numbers and operators. To this end, we define proxy <code class="language-plaintext highlighter-rouge">Scalar</code> and <code class="language-plaintext highlighter-rouge">ScalarFunction</code> classes that allow us to remember what operators were used on what numbers.</p> <p>Let’s consider an example function \(f(x, y) = x \* y + x\)</p> <pre><code class="language-mermaid">graph TD
    %% Input variables
    x[/"x"/]
    y[/"y"/]

    %% Operations
    mul1((Multiply))
    add((Add))

    %% Edges with values
    x --&gt; |x| mul1
    y --&gt; |y| mul1
    mul1 --&gt; |x*y| add
    x --&gt; |x| add
    add --&gt; |"x*y + x"| output[/"output"/]

    %% Styling
    classDef input fill:#e1f5fe,stroke:#01579b
    classDef operation fill:#fff3e0,stroke:#ff6f00
    classDef output fill:#e8f5e9,stroke:#1b5e20

    class x,y input
    class mul1,add operation
    class output output
</code></pre> <pre><code class="language-mermaid">graph TD
%% Input variables
x[/"x"/]
y[/"y"/]

    %% Operations
    mul1((Multiply))
    add((Add))

    %% Forward pass edges
    x --&gt; |"x"| mul1
    y --&gt; |"y"| mul1
    mul1 --&gt; |"x*y"| add
    x --&gt; |"x"| add
    add --&gt; |"x*y + x"| output[/"output"/]

    %% Backward pass edges (in red)
    output -.-&gt;|"d_output"| add
    add -.-&gt;|"d_output"| mul1
    add -.-&gt;|"d_output"| x
    mul1 -.-&gt;|"d_output * y"| x
    mul1 -.-&gt;|"d_output * x"| y

    %% Styling
    classDef input fill:#e1f5fe,stroke:#01579b
    classDef operation fill:#fff3e0,stroke:#ff6f00
    classDef output fill:#e8f5e9,stroke:#1b5e20

    %% Style for forward edges (solid)
    linkStyle 0,1,2,3,4 stroke:#2196f3,stroke-width:2px
    %% Style for backward edges (dashed)
    linkStyle 5,6,7,8,9 stroke:#f44336,stroke-width:2px,stroke-dasharray:3

    class x,y input
    class mul1,add operation
    class output output
</code></pre> <p>As an example, see how we define the multiplication operation. Note the usage of the context (ctx) variable to store values necessary for backpropagation:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Mul</span><span class="p">(</span><span class="n">ScalarFunction</span><span class="p">):</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">ctx</span><span class="p">.</span><span class="nf">save_for_backward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
        <span class="c1"># Compute f'_x(x, y) * d, f'_y(x, y) * d
</span>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">ctx</span><span class="p">.</span><span class="n">saved_values</span>
        <span class="n">f_x_prime</span> <span class="o">=</span> <span class="n">y</span>
        <span class="n">f_y_prime</span> <span class="o">=</span> <span class="n">x</span>
        <span class="k">return</span> <span class="n">f_x_prime</span> <span class="o">*</span> <span class="n">d</span><span class="p">,</span> <span class="n">f_y_prime</span> <span class="o">*</span> <span class="n">d</span>
</code></pre></div></div> <p>With this data saved, we can define a backpropagate function that actually computes the gradients. Note the usage of topological sort to obtain the order of operations. We also use the chain rule to calculate the gradients as we traverse the graph.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">backpropagate</span><span class="p">(</span><span class="n">variable</span><span class="p">:</span> <span class="n">Variable</span><span class="p">,</span> <span class="n">deriv</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Runs backpropagation on the computation graph in order to
    compute derivatives for the leave nodes.

    Args:
    ----
        variable: The right-most variable
        deriv  : Its derivative that we want to propagate backward to the leaves.

    Returns:
    -------
    No return. Should write to its results to the derivative values of each leaf through `accumulate_derivative`.

    </span><span class="sh">"""</span>
    <span class="c1"># Initialize a dictionary to store derivatives for each variable
</span>    <span class="n">derivatives</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="c1"># Perform topological sort
</span>    <span class="n">sorted_variables</span> <span class="o">=</span> <span class="nf">topological_sort</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span>
    <span class="n">derivatives</span><span class="p">[</span><span class="n">variable</span><span class="p">.</span><span class="n">unique_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">deriv</span>
    <span class="c1"># Iterate through the sorted variables
</span>    <span class="k">for</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">sorted_variables</span><span class="p">:</span>
        <span class="c1"># Get the current derivative for this variable
</span>        <span class="n">d_output</span> <span class="o">=</span> <span class="n">derivatives</span><span class="p">[</span><span class="n">var</span><span class="p">.</span><span class="n">unique_id</span><span class="p">]</span>

        <span class="c1"># If it's a leaf node, accumulate the derivative
</span>        <span class="k">if</span> <span class="n">var</span><span class="p">.</span><span class="nf">is_leaf</span><span class="p">():</span>
            <span class="n">var</span><span class="p">.</span><span class="nf">accumulate_derivative</span><span class="p">(</span><span class="n">d_output</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Apply the chain rule to get derivatives for parent variables
</span>            <span class="k">for</span> <span class="n">parent</span><span class="p">,</span> <span class="n">grad</span> <span class="ow">in</span> <span class="n">var</span><span class="p">.</span><span class="nf">chain_rule</span><span class="p">(</span><span class="n">d_output</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">parent</span><span class="p">.</span><span class="n">unique_id</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">derivatives</span><span class="p">:</span>
                    <span class="n">derivatives</span><span class="p">[</span><span class="n">parent</span><span class="p">.</span><span class="n">unique_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">grad</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">derivatives</span><span class="p">[</span><span class="n">parent</span><span class="p">.</span><span class="n">unique_id</span><span class="p">]</span> <span class="o">+=</span> <span class="n">grad</span>
</code></pre></div></div> <h2 id="part-3-tensors">Part 3: Tensors</h2> <p>So far, we’ve been working with scalars. But for neural networks with many parameters and weights, reasoning about each individual parameter is impractical. Instead, we define a new data structure: <code class="language-plaintext highlighter-rouge">tensors</code>, which multi-dimensional arrays that can serve as a basis for our mathematical programming.</p> <p>Why don’t we just use lists?</p> <ul> <li>We can define functions to manipulate shape</li> <li>Enables autodiff</li> <li>Enables control of memory</li> </ul> <p>Tensors have some properties: <code class="language-plaintext highlighter-rouge">dims</code>, <code class="language-plaintext highlighter-rouge">size</code>, <code class="language-plaintext highlighter-rouge">shape</code>. <code class="language-plaintext highlighter-rouge">dims</code> is the number of dimensions, <code class="language-plaintext highlighter-rouge">size</code> is the total number of elements, and <code class="language-plaintext highlighter-rouge">shape</code> is the size of each dimension.</p> <p>What are the cons of working with tensors?</p> <ul> <li>Hard to grow and shrink</li> <li>Lose python built-ins</li> <li>Tricky to reason about shapes</li> </ul> <p>Important note on the internals of tensors: we have a <code class="language-plaintext highlighter-rouge">Storage</code> that stores the actual data, and a <code class="language-plaintext highlighter-rouge">Tensor</code> class that wraps around the storage and provides a Python interface for interacting with the data.</p> <p>The <code class="language-plaintext highlighter-rouge">Storage</code> is a 1-D array of numbers of length <code class="language-plaintext highlighter-rouge">size</code>, and we have a <code class="language-plaintext highlighter-rouge">Strides</code> tuple that provides the mapping from user indices to indices in this 1-D storage. So, when we reshape or take transposes of tensors, the underlying data is not actually changed. Instead, our strides change.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/3dtensor-480.webp 480w,/assets/img/3dtensor-800.webp 800w,/assets/img/3dtensor-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/3dtensor.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Take this example of a 3D tensor (<code class="language-plaintext highlighter-rouge">dims</code> is 3, <code class="language-plaintext highlighter-rouge">size</code> is 12). We have shape <code class="language-plaintext highlighter-rouge">(2, 2, 3)</code>, and we have a <code class="language-plaintext highlighter-rouge">Strides</code> tuple <code class="language-plaintext highlighter-rouge">(6, 3, 1)</code>.</p> <p>We follow a convention for the shape to be depth, row, columns. To interpret the strides, the 6 is how many storage positions each proression in depth takes, 3 is the number of storage positions each progression in row takes, and 1 is the number of storage positions each progression in column takes.</p> <h2 id="part-4-efficiency">Part 4: Efficiency</h2> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/LimitsToPrediction/">Limits to Prediction</a> </li> </div> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2024 Evan Wang. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script>let theme=determineComputedTheme();document.onreadystatechange=(()=>{"complete"===document.readyState&&(document.querySelectorAll("pre>code.language-mermaid").forEach(e=>{const t=e.textContent,d=e.parentElement;d.classList.add("unloaded");let n=document.createElement("pre");n.classList.add("mermaid");const a=document.createTextNode(t);n.appendChild(a),d.after(n)}),mermaid.initialize({theme:theme}),"undefined"!=typeof d3&&window.addEventListener("load",function(){d3.selectAll(".mermaid svg").each(function(){var e=d3.select(this);e.html("<g>"+e.html()+"</g>");var t=e.select("g"),d=d3.zoom().on("zoom",function(e){t.attr("transform",e.transform)});e.call(d)})}))});</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>